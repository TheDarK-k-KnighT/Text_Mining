{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading given txt file and creating dataframe\n",
    "data_frame = pd.read_csv('C:\\\\Users\\\\The Dark Knight\\\\Desktop\\\\Applications\\\\Code\\\\Text_Mining\\\\User_Reviews_Data\\\\'\\\n",
    "                          'User_Restaurant_Reviews.txt', sep='\\t+', header = None, engine = \"python\")\n",
    "\n",
    "# Storing the dataframe in a csv file\n",
    "data_frame.to_csv('C:\\\\Users\\\\The Dark Knight\\\\Desktop\\\\Applications\\\\Code\\\\Text_Mining\\\\User_Reviews_Data\\\\'\\\n",
    "                  'User_Restaurant_Reviews.csv', index = None, header = ['Review','Sentiment'])\n",
    "\n",
    "# Reading the newly created csv file\n",
    "user_restaurant_reviews = pd.read_csv('C:\\\\Users\\\\The Dark Knight\\\\Desktop\\\\Applications\\\\Code\\\\Text_Mining\\\\'\\\n",
    "                  'User_Reviews_Data\\\\User_Restaurant_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the data\n",
    "user_restaurant_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3729, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_restaurant_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using only top 5 records for sample analysis\n",
    "limited_user_restaurant_reviews = user_restaurant_reviews[0:5]\n",
    "limited_user_restaurant_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Review', 'Sentiment'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_user_restaurant_reviews.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow... Loved this place.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize   #loading sentence and word tokenizing function\n",
    "\n",
    "sample_text = limited_user_restaurant_reviews[\"Review\"][0]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow...', 'Loved this place.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text into sentences using sent_tokenize\n",
    "sentence_tokens = sent_tokenize(sample_text)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', '...', 'Loved', 'this', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text into words using word_tokenize\n",
    "word_tokens = word_tokenize(sample_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "# Steps to remove stopwords (unimportant words) like a, an ,the, this, that, etc.\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', \"should've\", 'when', 'under', 'have', 'themselves', 'weren', 'where', 'herself', 'did', 'there', 'does', \"don't\", 'll', 'why', 'nor', \"she's\", 'doing', \"needn't\", 'yours', 'again', 'too', 'didn', 'your', 'were', 'a', 'because', 'him', 'then', 'from', \"mightn't\", 'of', \"aren't\", 'haven', 'you', 'any', 'very', 'm', 'above', 'but', 'during', 'an', 'most', 'the', 'than', 'below', 'yourself', 'just', 'wouldn', \"that'll\", 'needn', 't', 'himself', 'some', 'theirs', 'has', 'while', 'mustn', \"mustn't\", 'what', 'as', 'through', 'who', 'such', \"couldn't\", 'wasn', 'they', \"you'll\", 'on', 'at', 'don', 'no', 'couldn', 'ours', 'once', \"wouldn't\", 'aren', 'both', 'will', 'further', 'my', \"you'd\", 'off', 'against', 've', 'hers', 'shan', 'if', 'other', 'won', 'he', \"didn't\", \"hasn't\", 'whom', \"haven't\", 's', 'its', \"isn't\", 'itself', 'not', 'his', 'do', 'for', 'few', 'in', 'all', 'being', 'same', 'our', 'yourselves', 'down', 'after', 'to', 'over', \"won't\", 'her', 'more', 'myself', 'how', \"it's\", 'with', 'until', 'should', 'about', \"weren't\", 'hadn', 'shouldn', 'd', 'it', 'own', 'this', 'each', 're', 'y', 'before', 'or', 'me', 'hasn', 'had', 'up', 'having', \"you've\", \"hadn't\", \"wasn't\", 'i', 'was', 'these', 'we', 'am', \"doesn't\", 'them', 'ain', 'isn', 'only', 'are', 'o', 'which', \"shouldn't\", 'their', 'into', 'those', 'so', 'now', 'between', \"you're\", 'by', 'out', \"shan't\", 'and', 'she', 'can', 'be', 'ourselves', 'been', 'is', 'ma', 'here', 'that', 'mightn'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', \"should've\", 'when', 'under', 'have', 'themselves', 'weren', 'where', 'herself', 'did', 'there', 'does', \"don't\", 'll', '...', 'why', 'nor', \"she's\", 'doing', \"needn't\", 'yours', 'again', 'too', 'didn', 'your', 'were', 'a', 'because', 'him', 'then', 'from', \"mightn't\", 'of', \"aren't\", '....', 'haven', 'you', 'any', 'very', 'm', 'above', 'but', 'during', 'an', 'most', 'the', 'than', 'below', 'yourself', 'just', 'wouldn', \"that'll\", 'needn', 't', 'himself', 'some', 'theirs', 'has', 'while', 'mustn', \"mustn't\", 'what', 'as', 'through', 'who', 'such', \"couldn't\", 'wasn', 'they', \"you'll\", 'on', 'at', 'don', 'no', 'couldn', 'ours', 'once', \"wouldn't\", 'aren', 'both', 'will', 'further', 'my', \"you'd\", 'off', 'against', 've', 'hers', 'shan', 'if', 'other', 'won', 'he', \"didn't\", \"hasn't\", 'whom', \"haven't\", 's', 'its', \"isn't\", 'itself', 'not', 'his', 'do', 'for', 'few', 'in', 'all', 'being', 'same', 'our', 'yourselves', 'down', 'after', 'to', 'over', \"won't\", 'her', 'more', 'myself', 'how', \"it's\", 'with', 'until', 'should', 'about', \"weren't\", 'hadn', 'shouldn', 'd', 'it', 'own', 'this', 'each', 're', 'y', 'before', 'or', 'me', 'hasn', 'had', '..', '.', 'up', 'having', \"you've\", \"hadn't\", \"wasn't\", 'i', '.....', 'was', 'these', 'we', 'am', \"doesn't\", 'them', 'ain', 'isn', 'only', 'are', 'o', 'which', \"shouldn't\", 'their', 'into', 'those', 'so', 'now', 'between', \"you're\", 'by', 'out', \"shan't\", 'and', 'she', 'can', 'be', 'ourselves', 'been', 'is', 'ma', 'here', 'that', 'mightn'}\n"
     ]
    }
   ],
   "source": [
    "period_stop_words = {'.','..','...','....','.....'}\n",
    "stop_words = stop_words.union(period_stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'Loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "filtered_sample_text = [word for word in word_tokens if word not in stop_words]\n",
    "print(filtered_sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learned that if an electric slicer is used the blade becomes hot enough to start to cook the prosciutto.\n"
     ]
    }
   ],
   "source": [
    "# Using PorterStemmer to stem the words back to root word\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sample_text_2 = limited_user_restaurant_reviews[\"Review\"][1]\n",
    "print(sample_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens_2 = word_tokenize(sample_text_2)\n",
    "print(word_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'learn', 'that', 'if', 'an', 'electr', 'slicer', 'is', 'use', 'the', 'blade', 'becom', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "stem_tokens = [stemmer.stem(word) for word in word_tokens_2]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "# Using lemmatizer to keep meaningful words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in word_tokens_2]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_22_text = user_restaurant_reviews[\"Review\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after removing currency - \n",
      "We ordered     Margaritas they couldnt get the machine to work because it was frozen so refunded our money \n"
     ]
    }
   ],
   "source": [
    "# Using regex expressions to remove punctuations, symbols, currency etc.\n",
    "import re\n",
    "review_22_text_cleaned = re.sub(r'\\W+|\\d+|_',' ', review_22_text)\n",
    "print(\"Text after removing currency - \\n\" + review_22_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text - \n",
      "We ordered 2 $.99 Margaritas - they couldnt get the machine to work because it was frozen so refunded our money.\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual text - \\n\" + review_22_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
